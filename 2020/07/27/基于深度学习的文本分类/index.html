<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="google-site-verification" content="xO-bz1uwpqpuTJD3jdsW_HquC9kTqMpVQenlloR6D04" />
<meta name="baidu-site-verification" content="XS57ezyxze" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Pisces","version":"7.7.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="基于深度学习的文本分类学习目标PART 1  学习FastText的使用和基础原理 学会使用验证集进行调参  PART 2  学习Word2Vec的使用和基础原理 学习使用TextCNN、TextRNN进行文本表示 学习使用HAN网络结构完成文本分类  PART3  了解Transformer的原理和基于预训练语言模型（Bert）的词表示 学会Bert的使用，具体包括pretrain和finetu">
<meta property="og:type" content="article">
<meta property="og:title" content="基于深度学习的文本分类">
<meta property="og:url" content="http://yoursite.com/2020/07/27/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/index.html">
<meta property="og:site_name" content="ZQ的学习笔记">
<meta property="og:description" content="基于深度学习的文本分类学习目标PART 1  学习FastText的使用和基础原理 学会使用验证集进行调参  PART 2  学习Word2Vec的使用和基础原理 学习使用TextCNN、TextRNN进行文本表示 学习使用HAN网络结构完成文本分类  PART3  了解Transformer的原理和基于预训练语言模型（Bert）的词表示 学会Bert的使用，具体包括pretrain和finetu">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlly1gh583rqnywj30840403yj.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlly1gh58fsehspj308v04kaac.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlly1gha92si9hsj31b40r61e0.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlly1gha9rps08ij315y0nkjx6.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlly1gha9vl4dtsj30xy086go9.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlly1ghahqq4uqgj30ui0u0hdt.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlly1ghahr737z8j30vq0u0gqr.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlly1ghai4n54fnj31aw0igh26.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlly1ghai5vmroqj30u40u0e81.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlly1ghdkqi0j65j30u00xsn5l.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghdky1pkbbj31kj0egwi7.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlly1ghdl3e6qifj31b50ecdlp.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlly1ghdl6954igj30v40u07wh.jpg">
<meta property="article:published_time" content="2020-07-27T00:20:38.000Z">
<meta property="article:modified_time" content="2020-08-03T06:07:26.710Z">
<meta property="article:author" content="ZQ Liu">
<meta property="article:tag" content="datawhale">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://tva1.sinaimg.cn/large/007S8ZIlly1gh583rqnywj30840403yj.jpg">

<link rel="canonical" href="http://yoursite.com/2020/07/27/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>基于深度学习的文本分类 | ZQ的学习笔记</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

  <script src="https://cdn.jsdelivr.net/npm/jquery/dist/jquery.min.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome/css/font-awesome.min.css"/>

<link rel="alternate" href="/atom.xml" title="ZQ的学习笔记" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <script src="/live2d/autoload.js"></script>
  <div class="container use-motion">
    <div class="headband"></div>

    

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">ZQ的学习笔记</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
        <li class="menu-item menu-item-commonweal">

    <a href="/404/" rel="section"><i class="fa fa-fw fa-heartbeat"></i>公益 404</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/07/27/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="ZQ Liu">
      <meta itemprop="description" content="NLP、ML、DL">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ZQ的学习笔记">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          基于深度学习的文本分类
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-07-27 08:20:38" itemprop="dateCreated datePublished" datetime="2020-07-27T08:20:38+08:00">2020-07-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-08-03 14:07:26" itemprop="dateModified" datetime="2020-08-03T14:07:26+08:00">2020-08-03</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/datawhale/" itemprop="url" rel="index"><span itemprop="name">datawhale</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">



      
        <h2 id="基于深度学习的文本分类"><a href="#基于深度学习的文本分类" class="headerlink" title="基于深度学习的文本分类"></a>基于深度学习的文本分类</h2><h3 id="学习目标"><a href="#学习目标" class="headerlink" title="学习目标"></a>学习目标</h3><p>PART 1</p>
<ul>
<li>学习FastText的使用和基础原理</li>
<li>学会使用验证集进行调参</li>
</ul>
<p>PART 2</p>
<ul>
<li>学习Word2Vec的使用和基础原理</li>
<li>学习使用TextCNN、TextRNN进行文本表示</li>
<li>学习使用HAN网络结构完成文本分类</li>
</ul>
<p>PART3</p>
<ul>
<li>了解Transformer的原理和基于预训练语言模型（Bert）的词表示</li>
<li>学会Bert的使用，具体包括pretrain和finetune</li>
</ul>
<h3 id="FastText"><a href="#FastText" class="headerlink" title="FastText"></a>FastText</h3><h4 id="一、fastText简介"><a href="#一、fastText简介" class="headerlink" title="一、fastText简介"></a>一、fastText简介</h4><p>fastText是一个快速文本分类算法，与基于神经网络的分类算法相比有两大优点：<br>1、fastText在保持高精度的情况下加快了训练速度和测试速度<br>2、fastText不需要预训练好的词向量，fastText会自己训练词向量<br>3、fastText两个重要的优化：Hierarchical Softmax、N-gram</p>
<h4 id="二、fastText模型架构"><a href="#二、fastText模型架构" class="headerlink" title="二、fastText模型架构"></a>二、fastText模型架构</h4><p>fastText模型架构是一个三层的神经网络，输入层、隐含层和输出层，和word2vec中的CBOW很相似， 不同之处是fastText<strong>预测标签</strong>，CBOW预测的是<strong>中间词</strong>，即模型架构类似但是模型的任务不同。</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gh583rqnywj30840403yj.jpg" alt="img"></p>
<p>其中x~1~，x~2~，…，x~(N−1）~，x~N~表示一个文本中的n-gram向量，每个特征是词向量的平均值。</p>
<h4 id="三、层次softmax"><a href="#三、层次softmax" class="headerlink" title="三、层次softmax"></a>三、层次softmax</h4><p>softmax函数常在神经网络输出层充当激活函数，目的就是将输出层的值归一化到0-1区间，将神经元输出构造成概率分布。</p>
<p>在标准的softmax中，计算一个类别的softmax概率时，我们需要对所有类别概率做归一化，在这类别很大情况下非常耗时，因此提出了分层softmax(Hierarchical Softmax),思想是根据类别的频率构造霍夫曼树来代替标准softmax，通过分层softmax可以将复杂度从N降低到logN，下图给出分层softmax示例：</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gh58fsehspj308v04kaac.jpg" alt="img"></p>
<h4 id="四、N-gram特征"><a href="#四、N-gram特征" class="headerlink" title="四、N-gram特征"></a>四、N-gram特征</h4><p>基本思想是将文本内容按照子节顺序进行大小为N的窗口滑动操作，最终形成窗口为N的字节片段序列。n-gram可以根据粒度不同有不同的含义，有字粒度的n-gram和词粒度的n-gram。</p>
<p>优点：</p>
<p>1、为罕见的单词生成更好的单词向量</p>
<p>2、在词汇单词中，即使单词没有出现在训练语料库中，仍然可以从字符级n-gram中构造单词的词向量</p>
<p>3、n-gram可以让模型学习到局部单词顺序的部分信息，也可理解为上下文信息，</p>
<p>内存优化：</p>
<p>1、过滤掉出现次数少的单词<br> 2、使用hash存储<br> 3、由采用字粒度变化为采用词粒度</p>
<h3 id="基于FastText的文本分类"><a href="#基于FastText的文本分类" class="headerlink" title="基于FastText的文本分类"></a>基于FastText的文本分类</h3><h4 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h4><p>所有标签<code>__label__</code>均以前缀开头，这是fastText识别标签或单词的方式</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#转换格式</span></span><br><span class="line">train_df = pd.read_csv(<span class="string">'./train_set.csv'</span>, sep=<span class="string">'\t'</span>,nrows=<span class="number">15000</span>)</span><br><span class="line">train_df[<span class="string">'label_ft'</span>] = <span class="string">'__label__'</span> + train_df[<span class="string">'label'</span>].astype(str)</span><br><span class="line">train_df[[<span class="string">'text'</span>,<span class="string">'label_ft'</span>]].iloc[:<span class="number">-5000</span>].to_csv(<span class="string">'train.csv'</span>, index = <span class="literal">None</span>,header = <span class="literal">None</span>, sep=<span class="string">'\t'</span>)</span><br></pre></td></tr></table></figure>
<h4 id="分类器"><a href="#分类器" class="headerlink" title="分类器"></a>分类器</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> fasttext</span><br><span class="line">model = fasttext.train_supervised(<span class="string">'train.csv'</span>,</span><br><span class="line">                                  lr = <span class="number">1.0</span>,</span><br><span class="line">                                  dim = <span class="number">300</span>，</span><br><span class="line">                                  wordNgrams = <span class="number">2</span>,</span><br><span class="line">                                  verbose = <span class="number">2</span>,</span><br><span class="line">                                  minCount = <span class="number">1</span>,</span><br><span class="line">                                  epoch = <span class="number">25</span>,</span><br><span class="line">                                  loss = <span class="string">'hs'</span>)</span><br></pre></td></tr></table></figure>
<h6 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">input             # training file path (required)    </span><br><span class="line">lr                # learning rate [0.1]    </span><br><span class="line">dim               # size of word vectors [100]    </span><br><span class="line">ws                # size of the context window [5]    </span><br><span class="line">epoch             # number of epochs [5]    </span><br><span class="line">minCount          # minimal number of word occurences [1]</span><br><span class="line">minCountLabel     # minimal number of label occurences [1]</span><br><span class="line">minn              # min length of char ngram [0]    </span><br><span class="line">maxn              # max length of char ngram [0]    </span><br><span class="line">neg               # number of negatives sampled [5]    </span><br><span class="line">wordNgrams        # max length of word ngram [1]    </span><br><span class="line">loss              # loss function &#123;ns, hs, softmax, ova&#125; [softmax]    </span><br><span class="line">bucket            # number of buckets [2000000]    </span><br><span class="line">thread            # number of threads [number of cpus]    </span><br><span class="line">lrUpdateRate      # change the rate of updates for the learning rate [100]    </span><br><span class="line">t                 # sampling threshold [0.0001]</span><br><span class="line">label             # label prefix [&#39;__label__&#39;]</span><br><span class="line">verbose           # verbose [2]</span><br><span class="line">pretrainedVectors # pretrained word vectors (.vec file) for supervised learning []</span><br></pre></td></tr></table></figure>
<h4 id="F1"><a href="#F1" class="headerlink" title="F1"></a>F1</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val_pred = [model.predict(x)[<span class="number">0</span>][<span class="number">0</span>].split(<span class="string">'__'</span>)[<span class="number">-1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> train_df.iloc[<span class="number">-5000</span>:][<span class="string">'text'</span>]]</span><br><span class="line">s = f1_score(train_df[<span class="string">'label'</span>].values[<span class="number">-5000</span>:].astype(str),val_pred,average = <span class="string">'macro'</span>)</span><br><span class="line">print(s)</span><br></pre></td></tr></table></figure>
<blockquote>
<p><a href="https://github.com/datawhalechina/team-learning-nlp/blob/master/NewsTextClassification/Task4%20%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB1.md" target="_blank" rel="noopener">Task4 基于深度学习的文本分类1</a></p>
<p><a href="https://blog.csdn.net/feilong_csdn/article/details/88655927" target="_blank" rel="noopener">fastText原理和文本分类实战</a></p>
</blockquote>
<h3 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h3><p>word2vec的主要思路：通过单词和上下文彼此预测，对应的两个算法分别为：</p>
<ul>
<li>Skip-grams (SG)：预测上下文</li>
<li>Continuous Bag of Words (CBOW)：预测目标单词</li>
</ul>
<p>另外提出两种更加高效的训练方法：</p>
<ul>
<li>Hierarchical softmax</li>
<li>Negative sampling</li>
</ul>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gha92si9hsj31b40r61e0.jpg" alt="image-20200731164752291" style="zoom: 33%;" /></p>
<h4 id="Skip-grams"><a href="#Skip-grams" class="headerlink" title="Skip-grams"></a><strong>Skip-grams</strong></h4><p>对于句子：“The dog barked at the mailman”</p>
<p>skip_window=2 ，则，对于 barked ：将会得到 [‘The’, ‘dog’，’barked’, ‘at’, ’the’] .左侧2个词和右侧2个词。</p>
<p>num_skips=2， 将会得到 （’barked’，’dog’）,（’barken’，’at’）==&gt;[input,out]</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gha9rps08ij315y0nkjx6.jpg" alt="image-20200731171156500"></p>
<h5 id="Skip-grams训练"><a href="#Skip-grams训练" class="headerlink" title="Skip-grams训练"></a><strong>Skip-grams训练</strong></h5><p>input word和output word都会被我们进行one-hot编码。为了高效计算，它仅仅会选择矩阵中对应的向量中维度值为1的索引行：</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gha9vl4dtsj30xy086go9.jpg" alt="image-20200731171539993"></p>
<h6 id="Word-pairs-and-“phases"><a href="#Word-pairs-and-“phases" class="headerlink" title="Word pairs and “phases"></a>Word pairs and “phases</h6><p>将常见的单词组合（word pairs）或者词组作为单个“words”来处理</p>
<h6 id="对高频次单词进行抽样来减少训练样本的个数"><a href="#对高频次单词进行抽样来减少训练样本的个数" class="headerlink" title="对高频次单词进行抽样来减少训练样本的个数"></a>对高频次单词进行抽样来减少训练样本的个数</h6><p>​    对于“the”这种常用高频单词，我们将会有大量的（”the“，…）这样的训练样本，而这些样本数量远远超过了我们学习“the”这个词向量所需的训练样本数。</p>
<p>Word2Vec通过“抽样”模式来解决这种高频词问题。它的基本思想如下：对于我们在训练原始文本中遇到的每一个单词，它们都有一定概率被我们从文本中删掉，而这个被删除的概率与单词的频率有关。</p>
<p>ωi 是一个单词，Z(ωi) 是 ωi 这个单词在所有语料中出现的频次，例如：如果单词“peanut”在10亿规模大小的语料中出现了1000次，那么 Z(peanut) = 1000/1000000000 = 1e - 6。</p>
<p>P(ωi) 代表着保留某个单词的概率：$P\left(w_{i}\right)=(\sqrt{\frac{Z\left(w_{i}\right)}{0.001}}+1) \times \frac{0.001}{Z\left(w_{i}\right)}$</p>
<h6 id="负采样negative-sampling"><a href="#负采样negative-sampling" class="headerlink" title="负采样negative sampling"></a>负采样negative sampling</h6><p>对优化目标采用“negative sampling”方法，这样每个训练样本的训练只会更新一小部分的模型权重，从而降低计算负担</p>
<p>负采样（是用来提高训练速度并且改善所得到词向量的质量的一种方法。不同于原本每个训练样本更新所有的权重，负采样每次让一个训练样本仅仅更新一小部分的权重，这样就会降低梯度下降过程中的计算量。</p>
<p>当我们用训练样本 ( input word: “fox”，output word: “quick”) 来训练我们的神经网络时，“  fox”和“quick”都是经过one-hot编码的。如果我们的词典大小为10000时，在输出层，我们期望对应“quick”单词的那个神经元结点输出1，其余9999个都应该输出0。在这里，这9999个我们期望输出为0的神经元结点所对应的单词我们称为“negative” word。</p>
<p>当使用负采样时，我们将随机选择一小部分的negative words（比如选5个negative words）来更新对应的权重。我们也会对我们的“positive” word进行权重更新（在我们上面的例子中，这个单词指的是”quick“）。</p>
<p>对于小规模数据集，选择5-20个negative words会比较好，对于大规模数据集可以仅选择2-5个negative words。</p>
<p>我们使用“一元模型分布（unigram distribution）”来选择“negative words”。个单词被选作negative sample的概率跟它出现的频次有关，出现频次越高的单词越容易被选作negative words。</p>
<p>一个单词的负采样概率越大，那么它在这个表中出现的次数就越多，它被选中的概率就越大。</p>
<p>每个单词被选为“negative words”的概率计算公式：$P\left(w_{i}\right)=\frac{f\left(w_{i}\right)^{3 / 4}}{\sum_{j=0}^{n}\left(f\left(w_{j}\right)^{3 / 4}\right)}$</p>
<h5 id="Hierarchical-Softmax"><a href="#Hierarchical-Softmax" class="headerlink" title="Hierarchical Softmax"></a><em>Hierarchical Softmax</em></h5><p>为了避免要计算所有词的softmax概率，word2vec采样了霍夫曼树来代替从隐藏层到输出softmax层的映射。</p>
<p>霍夫曼树的建立：</p>
<ul>
<li>根据标签（label）和频率建立霍夫曼树（label出现的频率越高，Huffman树的路径越短）</li>
<li>Huffman树中每一叶子结点代表一个label</li>
</ul>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghahqq4uqgj30ui0u0hdt.jpg" alt="image-20200731214743683"></p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghahr737z8j30vq0u0gqr.jpg" alt="image-20200731214814429"></p>
<h4 id="使用gensim训练word2vec"><a href="#使用gensim训练word2vec" class="headerlink" title="使用gensim训练word2vec"></a><strong>使用gensim训练word2vec</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> gensim.models.word2vec <span class="keyword">import</span> Word2Vec</span><br><span class="line">model = Word2Vec(sentences, workers=num_workers, size=num_features)</span><br></pre></td></tr></table></figure>
<h3 id="TextCNN"><a href="#TextCNN" class="headerlink" title="TextCNN"></a>TextCNN</h3><p>采用了100个大小为2,3,4的卷积核，最后得到的文本向量大小为100*3=300维。</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghai4n54fnj31aw0igh26.jpg" alt="image-20200731220108081"></p>
<h3 id="TextRNN"><a href="#TextRNN" class="headerlink" title="TextRNN"></a>TextRNN</h3><p>TextRNN将句子中每个词的词向量依次输入到双向双层LSTM，分别将两个方向最后一个有效位置的隐藏层拼接成一个向量作为文本的表示。</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghai5vmroqj30u40u0e81.jpg" alt="image-20200731220217036"></p>
<h3 id="基于TextCNN、TextRNN的文本表示"><a href="#基于TextCNN、TextRNN的文本表示" class="headerlink" title="基于TextCNN、TextRNN的文本表示"></a>基于TextCNN、TextRNN的文本表示</h3><h4 id="TextCNN-1"><a href="#TextCNN-1" class="headerlink" title="TextCNN"></a>TextCNN</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">self.filter_sizes = [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]  <span class="comment"># n-gram window</span></span><br><span class="line">self.out_channel = <span class="number">100</span></span><br><span class="line">self.convs = nn.ModuleList([nn.Conv2d(<span class="number">1</span>, self.out_channel, (filter_size, input_size), bias=<span class="literal">True</span>) <span class="keyword">for</span> filter_size <span class="keyword">in</span> self.filter_sizes])</span><br><span class="line"></span><br><span class="line">pooled_outputs = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(self.filter_sizes)):</span><br><span class="line">    filter_height = sent_len - self.filter_sizes[i] + <span class="number">1</span></span><br><span class="line">    conv = self.convs[i](batch_embed)</span><br><span class="line">    hidden = F.relu(conv)  <span class="comment"># sen_num x out_channel x filter_height x 1</span></span><br><span class="line"></span><br><span class="line">    mp = nn.MaxPool2d((filter_height, <span class="number">1</span>))  <span class="comment"># (filter_height, filter_width)</span></span><br><span class="line">    <span class="comment"># sen_num x out_channel x 1 x 1 -&gt; sen_num x out_channel</span></span><br><span class="line">    pooled = mp(hidden).reshape(sen_num, self.out_channel)</span><br><span class="line">    </span><br><span class="line">    pooled_outputs.append(pooled)</span><br></pre></td></tr></table></figure>
<h4 id="TextRNN-1"><a href="#TextRNN-1" class="headerlink" title="TextRNN"></a>TextRNN</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">input_size &#x3D; config.word_dims</span><br><span class="line"></span><br><span class="line">self.word_lstm &#x3D; LSTM(</span><br><span class="line">    input_size&#x3D;input_size,</span><br><span class="line">    hidden_size&#x3D;config.word_hidden_size,</span><br><span class="line">    num_layers&#x3D;config.word_num_layers,</span><br><span class="line">    batch_first&#x3D;True,</span><br><span class="line">    bidirectional&#x3D;True,</span><br><span class="line">    dropout_in&#x3D;config.dropout_input,</span><br><span class="line">    dropout_out&#x3D;config.dropout_hidden,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">hiddens, _ &#x3D; self.word_lstm(batch_embed, batch_masks)  # sent_len x sen_num x hidden*2</span><br><span class="line">hiddens.transpose_(1, 0)  # sen_num x sent_len x hidden*2</span><br><span class="line"></span><br><span class="line">if self.training:</span><br><span class="line">    hiddens &#x3D; drop_sequence_sharedmask(hiddens, self.dropout_mlp)</span><br></pre></td></tr></table></figure>
<h3 id="使用HAN用于文本分类"><a href="#使用HAN用于文本分类" class="headerlink" title="使用HAN用于文本分类"></a>使用HAN用于文本分类</h3><p><a href="https://link.zhihu.com/?target=http%3A//www.aclweb.org/anthology/N16-1174">Hierarchical Attention Network for Document Classification</a>(HAN)基于层级注意力,在单词和句子级别分别编码并基于注意力获得文档的表示，然后经过Softmax进行分类。其中word encoder的作用是获得句子的表示.</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghdkqi0j65j30u00xsn5l.jpg" alt="image-20200803134803347"></p>
<h3 id="本章作业"><a href="#本章作业" class="headerlink" title="本章作业"></a>本章作业</h3><ul>
<li>尝试通过Word2Vec训练词向量</li>
<li>尝试使用TextCNN、TextRNN完成文本表示</li>
<li>尝试使用HAN进行文本分类</li>
</ul>
<h3 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h3><p>编码部分：结构完全相同，但是并不共享参数，每一个编码器都可以拆解成两部分。在对输入序列做词的向量化之后，它们首先流过一个self-attention层，该层帮助编码器在它编码单词的时候能够看到输入序列中的其他单词。self-attention的输出流向一个前向网络（Feed Forward Neural Network），每个输入位置对应的前向网络是独立互不干扰的。最后将输出传入下一个编码器。</p>
<p>关键特性：每个位置的词仅仅流过它自己的编码器路径。</p>
<h3 id="基于预训练语言模型的词表示"><a href="#基于预训练语言模型的词表示" class="headerlink" title="基于预训练语言模型的词表示"></a>基于预训练语言模型的词表示</h3><p>传统方法生成的单词映射表的形式，即先为每个单词生成一个静态的词向量，之后这个单词的表示就被固定住了，不会跟着上下文的变化而做出改变。</p>
<p>基于预训练语言模型的词表示可以建模上下文信息，解决传统静态词向量不能建模“一词多义”语言现象的问题</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghdky1pkbbj31kj0egwi7.jpg" alt=""></p>
<p>最早提出的ELMo基于两个单向LSTM，将从左到右和从右到左两个方向的隐藏层向量表示拼接学习上下文词嵌入。而GPT用Transformer代替LSTM作为编码器，首先进行了语言模型预训练，然后在下游任务微调模型参数。但GPT由于仅使用了单向语言模型，因此难以建模上下文信息。为了解决以上问题，研究者们提出了BERT，BERT模型结构如下图所示，它是一个基于Transformer的多层Encoder，通过执行一系列预训练，进而得到深层的上下文表示。</p>
<p>ELMo首先进行了语言模型预训练，然后在下游任务中动态调整Word Embedding，因此最后输出的词表示能够充分表达单词在上下文中的特定语义，进而解决一词多义的问题。</p>
<p>GPT来自于openai，是一种生成式预训练模型。GPT 除了将ELMo中的LSTM替换为Transformer  的Encoder外，更开创了NLP界基于预训练-微调的新范式。尽管GPT采用的也是和ELMo相同的两阶段模式，但GPT在第一个阶段并没有采取ELMo中使用两个单向双层LSTM拼接的结构，而是采用基于自回归式的单向语言模型。</p>
<p>与GPT相同，BERT也采用了预训练-微调这一两阶段模式。在模型结构方面，BERT采用了ELMO的方式式，即使用双向语言模型代替GPT中的单向语言模型。</p>
<p><strong>第一阶段</strong>的预训练过程中，BERT提出掩码语言模型，通过上下文来预测单词本身，而不是从右到左或从左到右建模，这允许模型能够自由地编码每个层中来自两个方向的信息；为了学习句子的词序关系，BERT将Transformer中的三角函数位置表示替换为可学习的参数；为了区别单句和双句输入，BERT还引入了句子类型表征。</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghdl3e6qifj31b50ecdlp.jpg" alt=""></p>
<p><strong>第二阶段</strong>，与GPT相同，BERT也使用Fine-Tuning模式来微调下游任务。并且极大的减少了改造下游任务的要求，只需在BERT模型的基础上，通过额外添加Linear分类器，就可以完成下游任务。</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghdl6954igj30v40u07wh.jpg" alt="image-20200803140309123"></p>
<h3 id="基于Bert的文本分类"><a href="#基于Bert的文本分类" class="headerlink" title="基于Bert的文本分类"></a>基于Bert的文本分类</h3><h4 id="Bert-Pretrain"><a href="#Bert-Pretrain" class="headerlink" title="Bert Pretrain"></a>Bert Pretrain</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WhitespaceTokenizer</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""WhitespaceTokenizer with vocab."""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_file)</span>:</span></span><br><span class="line">        self.vocab = load_vocab(vocab_file)</span><br><span class="line">        self.inv_vocab = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> self.vocab.items()&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tokenize</span><span class="params">(self, text)</span>:</span></span><br><span class="line">        split_tokens = whitespace_tokenize(text)</span><br><span class="line">        output_tokens = []</span><br><span class="line">        <span class="keyword">for</span> token <span class="keyword">in</span> split_tokens:</span><br><span class="line">            <span class="keyword">if</span> token <span class="keyword">in</span> self.vocab:</span><br><span class="line">                output_tokens.append(token)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                output_tokens.append(<span class="string">"[UNK]"</span>)</span><br><span class="line">        <span class="keyword">return</span> output_tokens</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">convert_tokens_to_ids</span><span class="params">(self, tokens)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> convert_by_vocab(self.vocab, tokens)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">convert_ids_to_tokens</span><span class="params">(self, ids)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> convert_by_vocab(self.inv_vocab, ids)</span><br></pre></td></tr></table></figure>
<p>预训练由于去除了NSP预训练任务，因此将文档处理多个最大长度为256的段，如果最后一个段的长度小于256/2则丢弃。每一个段执行按照BERT原文中执行掩码语言模型，然后处理成tfrecord格式。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_segments_from_document</span><span class="params">(document, max_segment_length)</span>:</span></span><br><span class="line">    <span class="string">"""Split single document to segments according to max_segment_length."""</span></span><br><span class="line">    <span class="keyword">assert</span> len(document) == <span class="number">1</span></span><br><span class="line">    document = document[<span class="number">0</span>]</span><br><span class="line">    document_len = len(document)</span><br><span class="line"></span><br><span class="line">    index = list(range(<span class="number">0</span>, document_len, max_segment_length))</span><br><span class="line">    other_len = document_len % max_segment_length</span><br><span class="line">    <span class="keyword">if</span> other_len &gt; max_segment_length / <span class="number">2</span>:</span><br><span class="line">        index.append(document_len)</span><br><span class="line"></span><br><span class="line">    segments = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(index) - <span class="number">1</span>):</span><br><span class="line">        segment = document[index[i]: index[i+<span class="number">1</span>]]</span><br><span class="line">        segments.append(segment)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> segments</span><br></pre></td></tr></table></figure>
<p>预训练过程中，也只执行掩码语言模型任务</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(masked_lm_loss, masked_lm_example_loss, masked_lm_log_probs) = get_masked_lm_output(</span><br><span class="line">    bert_config, model.get_sequence_output(), model.get_embedding_table(),</span><br><span class="line">    masked_lm_positions, masked_lm_ids, masked_lm_weights)</span><br><span class="line"></span><br><span class="line">total_loss = masked_lm_loss</span><br></pre></td></tr></table></figure>
<p>为了适配句子的长度，以及减小模型的训练时间，采取了BERT-mini模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="string">"hidden_size"</span>: <span class="number">256</span>,</span><br><span class="line">  <span class="string">"hidden_act"</span>: <span class="string">"gelu"</span>,</span><br><span class="line">  <span class="string">"initializer_range"</span>: <span class="number">0.02</span>,</span><br><span class="line">  <span class="string">"vocab_size"</span>: <span class="number">5981</span>,</span><br><span class="line">  <span class="string">"hidden_dropout_prob"</span>: <span class="number">0.1</span>,</span><br><span class="line">  <span class="string">"num_attention_heads"</span>: <span class="number">4</span>,</span><br><span class="line">  <span class="string">"type_vocab_size"</span>: <span class="number">2</span>,</span><br><span class="line">  <span class="string">"max_position_embeddings"</span>: <span class="number">256</span>,</span><br><span class="line">  <span class="string">"num_hidden_layers"</span>: <span class="number">4</span>,</span><br><span class="line">  <span class="string">"intermediate_size"</span>: <span class="number">1024</span>,</span><br><span class="line">  <span class="string">"attention_probs_dropout_prob"</span>: <span class="number">0.1</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>使用Pytorch</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert_tf_checkpoint_to_pytorch</span><span class="params">(tf_checkpoint_path, bert_config_file, pytorch_dump_path)</span>:</span></span><br><span class="line">    <span class="comment"># Initialise PyTorch model</span></span><br><span class="line">    config = BertConfig.from_json_file(bert_config_file)</span><br><span class="line">    print(<span class="string">"Building PyTorch model from configuration: &#123;&#125;"</span>.format(str(config)))</span><br><span class="line">    model = BertForPreTraining(config)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Load weights from tf checkpoint</span></span><br><span class="line">    load_tf_weights_in_bert(model, config, tf_checkpoint_path)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Save pytorch-model</span></span><br><span class="line">    print(<span class="string">"Save PyTorch model to &#123;&#125;"</span>.format(pytorch_dump_path))</span><br><span class="line">    torch.save(model.state_dict(), pytorch_dump_path)</span><br></pre></td></tr></table></figure>
<h4 id="Bert-Finetune"><a href="#Bert-Finetune" class="headerlink" title="Bert Finetune"></a>Bert Finetune</h4><p>微调将最后一层的第一个token即[CLS]的隐藏向量作为句子的表示，然后输入到softmax层进行分类。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">sequence_output, pooled_output = \</span><br><span class="line">    self.bert(input_ids=input_ids, token_type_ids=token_type_ids)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> self.pooled:</span><br><span class="line">    reps = pooled_output</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    reps = sequence_output[:, <span class="number">0</span>, :]  <span class="comment"># sen_num x 256</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> self.training:</span><br><span class="line">    reps = self.dropout(reps)</span><br></pre></td></tr></table></figure>
<h3 id="本章作业-1"><a href="#本章作业-1" class="headerlink" title="本章作业"></a>本章作业</h3><ul>
<li>完成Bert Pretrain和Finetune的过程</li>
<li>阅读Bert官方文档，找到相关参数进行调参</li>
</ul>

    </div>
    <div>
      
        
<div class="my_post_copyright">
  <script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>
  
  <!-- JS库 sweetalert 可修改路径 -->
  <script type="text/javascript" src="http://jslibs.wuxubj.cn/sweetalert_mini/jquery-1.7.1.min.js"></script>
  <script src="http://jslibs.wuxubj.cn/sweetalert_mini/sweetalert.min.js"></script>
  <link rel="stylesheet" type="text/css" href="http://jslibs.wuxubj.cn/sweetalert_mini/sweetalert.mini.css">

  <p><span>本文标题:</span>基于深度学习的文本分类</a></p>
  <p><span>文章作者:</span>ZQ Liu</a></p>
  <p><span>发布时间:</span>2020年07月27日 - 08:20:38</p>
  <p><span>最后更新:</span>2020年08月03日 - 14:07:26</p>
  <p><span>原始链接:</span><a href="/2020/07/27/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/" title="基于深度学习的文本分类">http://yoursite.com/2020/07/27/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/</a>
    <span class="copy-path"  title="点击复制文章链接"><i class="fa fa-clipboard" data-clipboard-text="http://yoursite.com/2020/07/27/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"  aria-label="复制成功！"></i></span>
  </p>
  <p><span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">署名-非商业性使用-禁止演绎 4.0 国际</a> 转载请保留原文链接及作者。</p>  
</div>
<script> 
    var clipboard = new Clipboard('.fa-clipboard');
    clipboard.on('success', $(function(){
      $(".fa-clipboard").click(function(){
        swal({   
          title: "",   
          text: '复制成功',   
          html: false,
          timer: 500,   
          showConfirmButton: false
        });
      });
    }));  
</script>

      
    </div>
    <div>
      
      <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>
      
     </div>



    
    
    
        

  <div class="followme">
    <p>欢迎关注我的其它发布渠道</p>

    <div class="social-list">

        <div class="social-item">
          <a target="_blank" class="social-link" href="/images/wechat_channel.jpg">
            <span class="icon">
              <i class="fa fa-wechat"></i>
            </span>

            <span class="label">WeChat</span>
          </a>
        </div>

        <div class="social-item">
          <a target="_blank" class="social-link" href="/atom.xml">
            <span class="icon">
              <i class="fa fa-rss"></i>
            </span>

            <span class="label">RSS</span>
          </a>
        </div>
    </div>
  </div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/datawhale/" rel="tag"># datawhale</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/07/25/%E5%9F%BA%E4%BA%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/" rel="prev" title="基于机器学习的文本分类">
      <i class="fa fa-chevron-left"></i> 基于机器学习的文本分类
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    

  </article>
  
  
  

  </div>


          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#基于深度学习的文本分类"><span class="nav-number">1.</span> <span class="nav-text">基于深度学习的文本分类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#学习目标"><span class="nav-number">1.1.</span> <span class="nav-text">学习目标</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#FastText"><span class="nav-number">1.2.</span> <span class="nav-text">FastText</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#一、fastText简介"><span class="nav-number">1.2.1.</span> <span class="nav-text">一、fastText简介</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#二、fastText模型架构"><span class="nav-number">1.2.2.</span> <span class="nav-text">二、fastText模型架构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#三、层次softmax"><span class="nav-number">1.2.3.</span> <span class="nav-text">三、层次softmax</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#四、N-gram特征"><span class="nav-number">1.2.4.</span> <span class="nav-text">四、N-gram特征</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#基于FastText的文本分类"><span class="nav-number">1.3.</span> <span class="nav-text">基于FastText的文本分类</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#准备数据"><span class="nav-number">1.3.1.</span> <span class="nav-text">准备数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#分类器"><span class="nav-number">1.3.2.</span> <span class="nav-text">分类器</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#参数"><span class="nav-number">1.3.2.0.1.</span> <span class="nav-text">参数</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#F1"><span class="nav-number">1.3.3.</span> <span class="nav-text">F1</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Word2Vec"><span class="nav-number">1.4.</span> <span class="nav-text">Word2Vec</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Skip-grams"><span class="nav-number">1.4.1.</span> <span class="nav-text">Skip-grams</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Skip-grams训练"><span class="nav-number">1.4.1.1.</span> <span class="nav-text">Skip-grams训练</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#Word-pairs-and-“phases"><span class="nav-number">1.4.1.1.1.</span> <span class="nav-text">Word pairs and “phases</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#对高频次单词进行抽样来减少训练样本的个数"><span class="nav-number">1.4.1.1.2.</span> <span class="nav-text">对高频次单词进行抽样来减少训练样本的个数</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#负采样negative-sampling"><span class="nav-number">1.4.1.1.3.</span> <span class="nav-text">负采样negative sampling</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Hierarchical-Softmax"><span class="nav-number">1.4.1.2.</span> <span class="nav-text">Hierarchical Softmax</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#使用gensim训练word2vec"><span class="nav-number">1.4.2.</span> <span class="nav-text">使用gensim训练word2vec</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#TextCNN"><span class="nav-number">1.5.</span> <span class="nav-text">TextCNN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#TextRNN"><span class="nav-number">1.6.</span> <span class="nav-text">TextRNN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#基于TextCNN、TextRNN的文本表示"><span class="nav-number">1.7.</span> <span class="nav-text">基于TextCNN、TextRNN的文本表示</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#TextCNN-1"><span class="nav-number">1.7.1.</span> <span class="nav-text">TextCNN</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#TextRNN-1"><span class="nav-number">1.7.2.</span> <span class="nav-text">TextRNN</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用HAN用于文本分类"><span class="nav-number">1.8.</span> <span class="nav-text">使用HAN用于文本分类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#本章作业"><span class="nav-number">1.9.</span> <span class="nav-text">本章作业</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Transformer"><span class="nav-number">1.10.</span> <span class="nav-text">Transformer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#基于预训练语言模型的词表示"><span class="nav-number">1.11.</span> <span class="nav-text">基于预训练语言模型的词表示</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#基于Bert的文本分类"><span class="nav-number">1.12.</span> <span class="nav-text">基于Bert的文本分类</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Bert-Pretrain"><span class="nav-number">1.12.1.</span> <span class="nav-text">Bert Pretrain</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Bert-Finetune"><span class="nav-number">1.12.2.</span> <span class="nav-text">Bert Finetune</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#本章作业-1"><span class="nav-number">1.13.</span> <span class="nav-text">本章作业</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">ZQ Liu</p>
  <div class="site-description" itemprop="description">NLP、ML、DL</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">13</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">分类</span>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/ZeQing-Liu" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;ZeQing-Liu" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zeqing.liu@qq.com" title="E-Mail → mailto:zeqing.liu@qq.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">ZQ Liu</span>
</div>
<div class="powered-by">
<i class="fa fa-user-md"></i><span id="busuanzi_container_site_pv">
    本站总访问量<span id="busuanzi_value_site_pv"></span>
</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.2.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.7.2
  </div>

        








      </div>
    </footer>
  </div>

  
  <script size="300" alpha="0.6" zIndex="-1" src="/lib/canvas-ribbon/canvas-ribbon.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

  <!-- 页面点击小红心 -->
  <script type="text/javascript" src="/js/src/love.js"></script>

</body>
</html>
